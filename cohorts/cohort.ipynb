{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SQLContext\n",
    "import json\n",
    "import math\n",
    "import arrow\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"cohort\") \\\n",
    "    .config(\"spark.some.config.option\", \"cohort\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def getDatetimeFromEpoch(epoch=None):\n",
    "    if epoch is not None:\n",
    "        epoch = epoch/1000\n",
    "        utc_date = arrow.get(epoch)\n",
    "        local = utc_date.to('Asia/Kolkata')\n",
    "        local = local.datetime\n",
    "        return local\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def getCohortGroupId(action_time):\n",
    "    return \"week\" + str(getWeekNumberFromDateTime(action_time))\n",
    "\n",
    "def getWeekNumberFromDateTime(action_time):\n",
    "    return getDatetimeFromEpoch(action_time).strftime(\"%V\")\n",
    "\n",
    "def removeUtmParam(utmString):\n",
    "    return utmString.replace(\"utm_source:\", \"\")\n",
    "\n",
    "def week22(row):\n",
    "    return \"week22\" == row[1]\n",
    "\n",
    "def week23(row):\n",
    "    return \"week23\" == row[1]\n",
    "\n",
    "def week24(row):\n",
    "    return \"week24\" == row[1]\n",
    "\n",
    "def week25(row):\n",
    "    return \"week25\" == row[1]\n",
    "\n",
    "def week26(row):\n",
    "    return \"week26\" == row[1]\n",
    "\n",
    "cohort_map = {\"Criteo\":{}, \"google\":{}, \"Facebook\":{}, \"Email\":{}, \"sms\":{}}\n",
    "file_path = \"/home/marino/Desktop/website_events.csv\"\n",
    "df  = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n",
    "track_events = df.where(df[\"eventType\"] == \"UTM_PARAMS\")\n",
    "view_events = df.where(df[\"eventType\"] == \"PRODUCT_VIEW\")\n",
    "df1 = track_events.alias('df1')\n",
    "df2 = view_events.alias('df2')\n",
    "track_plus_view_events = df1.join(df2, df1.userId == df2.userId).select(['df1.productId', 'df2.userId', 'df2.eventTime'])\n",
    "overall_view_actions = view_events.rdd.map((lambda x: (x[\"userId\"], getCohortGroupId(int(x[\"eventTime\"])))))\n",
    "cohort = track_plus_view_events.rdd.filter((lambda x: \"utm_source\" in x[\"productId\"])).map((lambda x: (removeUtmParam(x[\"productId\"]), x[\"userId\"], getCohortGroupId(int(x[\"eventTime\"])))))\n",
    "cohort_df = cohort.toDF([\"source\", \"userId\", \"cohortId\"])\n",
    "cohort_df.write.partitionBy(\"source\").csv(\"cohorts\")\n",
    "\n",
    "sources =  [ \"google\", \"Facebook\", \"Email\", \"sms\"] \n",
    "for source in sources:\n",
    "    print 'loading ' + source +  \" cohort \"\n",
    "    cohortGroup  = spark.read.format(\"csv\").load(\"cohorts/source=\" + source)\n",
    "    cohortGroup_rdd = cohortGroup.rdd.map(lambda x: (x._c0, x._c1))\n",
    "    week22_rdd, week23_rdd, week24_rdd, week25_rdd, week26_rdd = (cohortGroup_rdd.filter(f).map(lambda f: f[0]).distinct() for f in (week22, week23, week24, week25, week26))\n",
    "    cohort_size = week22_rdd.count() \n",
    "    print \"Cohort size\", cohort_size\n",
    "    cohort_map[source][\"week0\"] = 100\n",
    "    cohort_map[source][\"week1\"] = (week22_rdd.intersection(week23_rdd).count() * 100 ) / cohort_size\n",
    "    cohort_map[source][\"week2\"] = (week22_rdd.intersection(week24_rdd).count() * 100) / cohort_size\n",
    "    cohort_map[source][\"week3\"] = (week22_rdd.intersection(week25_rdd).count() * 100) / cohort_size\n",
    "    cohort_map[source][\"week4\"] = (week22_rdd.intersection(week26_rdd).count()* 100 ) / cohort_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'path file:/home/marino/spark/cohorts already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2e21100218e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcohort_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcohort\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"userId\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cohortId\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcohort_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cohorts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/marino/.local/lib/python2.7/site-packages/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping)\u001b[0m\n\u001b[1;32m    881\u001b[0m                        \u001b[0mignoreTrailingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignoreTrailingWhiteSpace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m                        charToEscapeQuoteEscaping=charToEscapeQuoteEscaping)\n\u001b[0;32m--> 883\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/marino/.local/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/marino/.local/lib/python2.7/site-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'path file:/home/marino/spark/cohorts already exists.;'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sms cohort \n",
      "Cohort size 182\n",
      "{'sms': {'week1': 59, 'week0': 100, 'week3': 38, 'week2': 51, 'week4': 45}, 'Criteo': {}, 'google': {}, 'Facebook': {}, 'Email': {}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print cohort_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
